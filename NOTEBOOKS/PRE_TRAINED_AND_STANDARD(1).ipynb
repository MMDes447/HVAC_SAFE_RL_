{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZTQ0rzBjy_z"
      },
      "outputs": [],
      "source": [
        "!pip install numpy==1.24.4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jax==0.4.20 jaxlib==0.4.20"
      ],
      "metadata": {
        "id": "4Q-irkhokVMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium==0.28.1"
      ],
      "metadata": {
        "id": "CcOFYeU5kVOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  !rm -rf boptestGymService\n",
        "except:\n",
        "  pass\n",
        "!git clone -b boptest-gym-service https://github.com/ibpsa/project1-boptest-gym.git boptestGymService"
      ],
      "metadata": {
        "id": "xTlu3I6XkVQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# url for the BOPTEST service\n",
        "url = 'https://api.boptest.net'\n",
        "\n",
        "# Select test case and get identifier\n",
        "testcase = 'bestest_hydronic_heat_pump'\n",
        "\n",
        "# Select and start a new test case\n",
        "testid = \\\n",
        "requests.post('{0}/testcases/{1}/select'.format(url,testcase)).json()['testid']"
      ],
      "metadata": {
        "id": "OzSzXc7CkVTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(0,'boptestGymService')\n",
        "from boptestGymEnv import BoptestGymEnv"
      ],
      "metadata": {
        "id": "0yzomgGwkVWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from boptestGymEnv import BoptestGymEnv, NormalizedObservationWrapper, DiscretizedActionWrapper\n",
        "import os\n",
        "import numpy as np\n",
        "import torch as T\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions.categorical import Categorical"
      ],
      "metadata": {
        "id": "c-94I3egkVbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PPOMemory1:\n",
        "  def __init__(self, batch_size):\n",
        "     self.states=[]\n",
        "     self.probs=[]\n",
        "     self.vals=[]\n",
        "     self.actions=[]\n",
        "     self.rewards=[]\n",
        "     self.dones=[]\n",
        "\n",
        "     self.batch_size=batch_size\n",
        "\n",
        "  def generate_batches(self):\n",
        "    n_states=len(self.states)\n",
        "    batch_start=np.arange(0,n_states,self.batch_size)\n",
        "    indices=np.arange(n_states, dtype=np.int64)\n",
        "    np.random.shuffle(indices)\n",
        "    batches=[indices[i:i+self.batch_size] for i in batch_start]\n",
        "    return np.array(self.states),np.array(self.actions),np.array(self.probs),np.array(self.vals),np.array(self.rewards),np.array(self.dones),batches\n",
        "\n",
        "  def store_memory(self,state,action,probs,vals,reward,done):\n",
        "    self.states.append(state)\n",
        "    self.actions.append(action)\n",
        "    self.probs.append(probs)\n",
        "    self.vals.append(vals)\n",
        "    self.rewards.append(reward)\n",
        "    self.dones.append(done)\n",
        "\n",
        "  def clear_memory(self):\n",
        "      self.states = []\n",
        "      self.probs = []\n",
        "      self.actions = []\n",
        "      self.rewards = []\n",
        "      self.dones = []\n",
        "      self.vals = []\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Zra3A4b0kVg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ActorNetwork1(nn.Module):\n",
        "  def __init__(self,n_actions,input_dims,alpha,fc1_dims=256,fc2_dims=256,chkpt_dir='tmp/ppo'):\n",
        "    super(ActorNetwork1, self).__init__()\n",
        "    self.checkpoint_file=os.path.join(chkpt_dir,'actor_ppo')\n",
        "    self.actor=nn.Sequential(\n",
        "        nn.Linear(input_dims,fc1_dims),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(fc1_dims,fc2_dims),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(fc2_dims,n_actions),\n",
        "        nn.Softmax(dim=-1)\n",
        "    )\n",
        "    self.optimizer=optim.Adam(self.parameters(),lr=alpha)\n",
        "    self.device=T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
        "    self.to(self.device)\n",
        "  def forward(self, state):\n",
        "    dist=self.actor(state)\n",
        "    dist=Categorical(dist)\n",
        "    return dist\n",
        "  def save_checkpoint(self):\n",
        "    T.save(self.state_dict(), self.checkpoint_file)\n",
        "\n",
        "  def load_checkpoint(self):\n",
        "    self.load_state_dict(T.load(self.checkpoint_file))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7XKTiwkckWFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CriticNetwork1(nn.Module):\n",
        "    def __init__(self, input_dims, alpha, fc1_dims=256, fc2_dims=256,\n",
        "            chkpt_dir='tmp/ppo'):\n",
        "        super(CriticNetwork1, self).__init__()\n",
        "\n",
        "        self.checkpoint_file = os.path.join(chkpt_dir, 'critic_torch_ppo')\n",
        "        self.critic = nn.Sequential(\n",
        "                nn.Linear(input_dims, fc1_dims),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(fc1_dims, fc2_dims),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(fc2_dims, 1)\n",
        "        )\n",
        "\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
        "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, state):\n",
        "        value = self.critic(state)\n",
        "\n",
        "        return value\n",
        "\n",
        "    def save_checkpoint(self):\n",
        "        T.save(self.state_dict(), self.checkpoint_file)\n",
        "\n",
        "    def load_checkpoint(self):\n",
        "        self.load_state_dict(T.load(self.checkpoint_file))"
      ],
      "metadata": {
        "id": "P1QkS9ZNkWIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BarrierNet(nn.Module):\n",
        "  def __init__(self, input_dims):\n",
        "    super(BarrierNet, self).__init__()\n",
        "    self.netwrok=nn.Sequential(nn.Linear(input_dims,256),\n",
        "                               nn.Tanh(),\n",
        "                               nn.Linear(256,256),\n",
        "                               nn.Tanh(),\n",
        "                               nn.Linear(256,1)\n",
        "                               )\n",
        "    self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
        "    self.to(self.device)\n",
        "  def forward(self,state):\n",
        "    return self.netwrok(state)"
      ],
      "metadata": {
        "id": "y-3zxunKkWNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PPOAgent1_epch:\n",
        "    def __init__(self, n_actions, input_dims, alpha=0.0003, batch_size=64,\n",
        "                 n_epochs=10, gae_lambda=0.95, gamma=0.99, policy_clip=0.2):\n",
        "        self.gamma = gamma\n",
        "        self.policy_clip = policy_clip\n",
        "        self.n_epochs = n_epochs\n",
        "        self.gae_lambda = gae_lambda\n",
        "        self.batch_size = batch_size\n",
        "        self.global_step = 0\n",
        "        self.epsilon = .01\n",
        "\n",
        "        # PPO networks\n",
        "        self.actor = ActorNetwork1(n_actions, input_dims, alpha)\n",
        "        self.critic = CriticNetwork1(input_dims, alpha)\n",
        "        self.memory = PPOMemory1(batch_size)\n",
        "\n",
        "        # Barrier components\n",
        "        self.barrier = BarrierNet(input_dims)\n",
        "        self.barrier_optimizer = optim.Adam(self.barrier.parameters(), lr=0.001)\n",
        "        self.lambda_param = 0.01\n",
        "        self.barrier_memory = {\n",
        "            'initial_states': [],\n",
        "            'unsafe_states': [],\n",
        "            'states': [],\n",
        "            'next_states': []\n",
        "        }\n",
        "\n",
        "        # Lagrangian optimization parameters\n",
        "        self.nu=T.tensor(1.00, requires_grad=True)\n",
        "        self.alpha_dual = .01  # Learning rate for dual ascent\n",
        "        self.nu_optimizer= optim.Adam([self.nu],lr=self.alpha_dual)\n",
        "        # Temperature bounds (if needed)\n",
        "        self.lower_temp = 18 + 273.15\n",
        "        self.upper_temp = 21 + 273.15\n",
        "\n",
        "    def is_safe(self, state):\n",
        "        return ((-1 <= state[0] <= 1 and state[1]!=-1) or (-3 <= state[0] <= 7 and state[1]==-1))\n",
        "\n",
        "    def is_unsafe(self, state):\n",
        "        return not self.is_safe(state)\n",
        "\n",
        "\n",
        "    def remember(self, state, action, probs, vals, reward, done, next_state):\n",
        "        # PPO memory\n",
        "        self.memory.store_memory(state, action, probs, vals, reward, done)\n",
        "\n",
        "        # Barrier memory\n",
        "        state_arr = np.array(state)\n",
        "        next_state_arr = np.array(next_state)\n",
        "\n",
        "        self.barrier_memory['states'].append(state_arr)\n",
        "        # self.barrier_memory['next_states'].append(next_state_arr)\n",
        "\n",
        "        if self.is_unsafe(state_arr):\n",
        "            self.barrier_memory['unsafe_states'].append(state_arr)\n",
        "        if self.is_safe(state_arr):\n",
        "            self.barrier_memory['initial_states'].append(state_arr)\n",
        "\n",
        "        if not done:  # Only add next_state if episode isn't done\n",
        "            self.barrier_memory['next_states'].append(next_state_arr)\n",
        "\n",
        "    def barrier_feasible_loss(self, initial_states):\n",
        "        barrier_vals = self.barrier(initial_states)\n",
        "        return T.mean(T.max(barrier_vals, T.zeros_like(barrier_vals)))\n",
        "\n",
        "    def barrier_infeasible_loss(self, unsafe_states):\n",
        "        barrier_vals = self.barrier(unsafe_states)\n",
        "        return T.mean(T.max(-barrier_vals, T.zeros_like(barrier_vals)))\n",
        "\n",
        "    def barrier_invariant_loss(self, states, next_states):\n",
        "        current_barrier = self.barrier(states)\n",
        "        next_barrier = self.barrier(next_states)\n",
        "        return T.mean(\n",
        "           T.max(next_barrier - (1 - self.lambda_param) * current_barrier,\n",
        "                 T.zeros_like(current_barrier))\n",
        "        )\n",
        "\n",
        "    def learn_barrier(self):\n",
        "        if (len(self.barrier_memory['initial_states']) > 0 and\n",
        "            len(self.barrier_memory['unsafe_states']) > 0 and\n",
        "            len(self.barrier_memory['next_states']) > 0):\n",
        "            bar_batch_siz=6\n",
        "            bar_epoch=10\n",
        "            min_samples = min(\n",
        "            len(self.barrier_memory['initial_states']),\n",
        "            len(self.barrier_memory['unsafe_states']),\n",
        "            len(self.barrier_memory['states']),\n",
        "            len(self.barrier_memory['next_states']))\n",
        "            num_transitions = len(self.barrier_memory['next_states'])\n",
        "            # if min_samples == 0:\n",
        "            #   return None\n",
        "\n",
        "            # if min_samples < bar_batch_siz:\n",
        "            #   bar_batch_siz=min_samples\n",
        "\n",
        "            # tot_sample=min_samples\n",
        "            # indic=np.arange(tot_sample)\n",
        "            for _ in range(bar_epoch):\n",
        "              # np.random.shuffle(indic)\n",
        "              epoch_loss=0\n",
        "              # # for start_idx in range(0,tot_sample,bar_batch_siz):\n",
        "              #   batch_indic=indic[start_idx:start_idx+bar_batch_siz]\n",
        "\n",
        "\n",
        "              #   num_transitions = len(self.barrier_memory['next_states'])\n",
        "              #   initial_states=T.tensor(np.array([self.barrier_memory['initial_states'][i] for i in batch_indic]),dtype=T.float).to(self.barrier.device)\n",
        "              #   unsafe_states=T.tensor(np.array([self.barrier_memory['unsafe_states'][i] for i in batch_indic]),dtype=T.float).to(self.barrier.device)\n",
        "              #   states=T.tensor(np.array([self.barrier_memory['states'][i] for i in batch_indic]),dtype=T.float).to(self.barrier.device)\n",
        "              #   next_states=T.tensor(np.array([self.barrier_memory['next_states'][i] for i in batch_indic]),dtype=T.float).to(self.barrier.device)\n",
        "\n",
        "\n",
        "              initial_states = T.tensor(np.array(self.barrier_memory['initial_states']),\n",
        "                                      dtype=T.float).to(self.barrier.device)\n",
        "              unsafe_states = T.tensor(np.array(self.barrier_memory['unsafe_states']),\n",
        "                                      dtype=T.float).to(self.barrier.device)\n",
        "              states = T.tensor(np.array(self.barrier_memory['states'][:num_transitions]),\n",
        "                                dtype=T.float).to(self.barrier.device)\n",
        "              next_states = T.tensor(np.array(self.barrier_memory['next_states']),\n",
        "                                    dtype=T.float).to(self.barrier.device)\n",
        "\n",
        "              self.barrier_optimizer.zero_grad()\n",
        "              barrier_loss = (self.barrier_feasible_loss(initial_states) +\n",
        "                              self.barrier_infeasible_loss(unsafe_states) +\n",
        "                              self.barrier_invariant_loss(states, next_states))\n",
        "              barrier_loss.backward()\n",
        "              self.barrier_optimizer.step()\n",
        "              epoch_loss+=barrier_loss.item()\n",
        "              print(f'Barrier loss: {barrier_loss.item()}')\n",
        "              # final_loss=epoch_loss/tot_sample\n",
        "              # print(f'Barrier loss: {final_loss}')\n",
        "\n",
        "            # return epoch_loss/bar_epoch\n",
        "        # return barrier_loss.item()\n",
        "\n",
        "    def choose_action(self, observation):\n",
        "        state = T.tensor([observation], dtype=T.float).to(self.actor.device)\n",
        "\n",
        "        dist = self.actor(state)\n",
        "        value = self.critic(state)\n",
        "        action = dist.sample()\n",
        "\n",
        "        probs = T.squeeze(dist.log_prob(action)).item()\n",
        "        action = T.squeeze(action).item()\n",
        "        value = T.squeeze(value).item()\n",
        "\n",
        "        return action, probs, value\n",
        "\n",
        "    # def learn_lag(self,prob_ratio):\n",
        "    #     num_transitions = len(self.barrier_memory['next_states'])\n",
        "    #     states = T.tensor(np.array(self.barrier_memory['states'][-1]),\n",
        "    #                         dtype=T.float).to(self.barrier.device)\n",
        "    #     next_states = T.tensor(np.array(self.barrier_memory['next_states'][-1]),\n",
        "    #     dtype=T.float).to(self.barrier.device)\n",
        "\n",
        "    #     J_invt = self.barrier_invariant_loss(states, next_states)\n",
        "\n",
        "    #     # Weight the invariant loss with importance sampling ratio\n",
        "    #     importance_weighted_J_invt = prob_ratio.detach() * J_invt\n",
        "\n",
        "    #     # Calculate gradient for dual ascent\n",
        "    #     nu_grad = importance_weighted_J_invt.mean().detach()\n",
        "\n",
        "    #     # Update nu using dual ascent and project onto non-negative orthant\n",
        "    #     self.nu = T.clamp(self.nu - self.alpha_dual * nu_grad, min=0.0)\n",
        "\n",
        "    #     return self.nu.item()\n",
        "\n",
        "\n",
        "    def learn(self):\n",
        "        # self.train_step_count = getattr(self, 'train_step_count', 0)\n",
        "        for _ in range(self.n_epochs):\n",
        "\n",
        "            state_arr, action_arr, old_prob_arr, vals_arr, reward_arr, done_arr, batches = \\\n",
        "                self.memory.generate_batches()\n",
        "\n",
        "            values = vals_arr\n",
        "            advantage = np.zeros(len(reward_arr), dtype=np.float32)\n",
        "\n",
        "            # GAE calculation\n",
        "            for t in range(len(reward_arr)-1):\n",
        "                discount = 1\n",
        "                a_t = 0\n",
        "                for k in range(t, len(reward_arr)-1):\n",
        "                    a_t += discount*(reward_arr[k] + self.gamma*values[k+1]*\n",
        "                    (1-int(done_arr[k])) - values[k])\n",
        "                    discount *= self.gamma*self.gae_lambda\n",
        "                advantage[t] = a_t\n",
        "\n",
        "            advantage = T.tensor(advantage).to(self.actor.device)\n",
        "            values = T.tensor(values).to(self.actor.device)\n",
        "\n",
        "            # PPO update with Lagrangian constraint\n",
        "            for batch in batches:\n",
        "                states = T.tensor(state_arr[batch], dtype=T.float).to(self.actor.device)\n",
        "                old_probs = T.tensor(old_prob_arr[batch]).to(self.actor.device)\n",
        "                actions = T.tensor(action_arr[batch]).to(self.actor.device)\n",
        "\n",
        "                # Get next states for the batch\n",
        "                next_indices = [min(i + 1, len(state_arr) - 1) for i in batch]\n",
        "                next_states = T.tensor(state_arr[next_indices], dtype=T.float).to(self.actor.device)\n",
        "\n",
        "                dist = self.actor(states)\n",
        "                critic_value = self.critic(states)\n",
        "                critic_value = T.squeeze(critic_value)\n",
        "\n",
        "                new_probs = dist.log_prob(actions)\n",
        "\n",
        "                # Calculate importance sampling ratio\n",
        "                prob_ratio = new_probs.exp()/old_probs.exp()\n",
        "\n",
        "                # PPO actor loss components\n",
        "                weighted_probs = advantage[batch]*prob_ratio\n",
        "                weighted_clipped_probs = T.clamp(prob_ratio, 1-self.policy_clip,\n",
        "                1+self.policy_clip)*advantage[batch]\n",
        "\n",
        "                # Original PPO actor loss\n",
        "                actor_loss = -T.min(weighted_probs, weighted_clipped_probs).mean()\n",
        "\n",
        "                # Calculate invariant loss with importance sampling\n",
        "                J_invt = self.barrier_invariant_loss(states, next_states).detach()\n",
        "\n",
        "\n",
        "                # Update Lagrange multiplier (dual ascent)\n",
        "                # nu_grad = importance_weighted_J_invt.mean().detach()\n",
        "                # self.nu = T.clamp(self.nu - self.alpha_dual * nu_grad, min=0.0)\n",
        "\n",
        "                # Combine with invariant constraint using Lagrangian\n",
        "                # print(nu)\n",
        "\n",
        "                importance_weighted_J_invt = (prob_ratio * J_invt).mean()\n",
        "\n",
        "                constrained_actor_loss = actor_loss + (importance_weighted_J_invt+.001)*(self.nu.detach())\n",
        "                # print(self.nu * importance_weighted_J_invt.mean())\n",
        "                # Critic loss\n",
        "                returns = advantage[batch] + values[batch]\n",
        "                critic_loss = (returns-critic_value)**2\n",
        "                critic_loss = critic_loss.mean()\n",
        "\n",
        "                self.actor.optimizer.zero_grad()\n",
        "                self.critic.optimizer.zero_grad()\n",
        "                constrained_actor_loss.backward()\n",
        "                critic_loss.backward()\n",
        "                # actor_loss.backward()\n",
        "                self.actor.optimizer.step()\n",
        "                self.critic.optimizer.step()\n",
        "                #nu_update\n",
        "                # if self.global_step % 10 == 0:\n",
        "                self.nu_optimizer.zero_grad()\n",
        "                loss_nu = -self.nu * (importance_weighted_J_invt.detach()+.001)\n",
        "                loss_nu.backward()\n",
        "                # T.nn.utils.clip_grad_norm_([self.nu], max_norm=1.0)\n",
        "                self.nu_optimizer.step()\n",
        "                T.clamp(self.nu,min=0.0)\n",
        "                # if self.global_step % 20 ==0:\n",
        "                self.barrier_loss=self.learn_barrier()\n",
        "                self.global_step+=1\n",
        "                print(self.nu.item())\n",
        "                # print(loss_nu)\n",
        "                print(J_invt)\n",
        "                print(actor_loss)\n",
        "                self.log_training_metrics(\n",
        "                writer,  # You'll need to pass the writer to learn()\n",
        "                self.global_step,  # You'll need to pass the current step count\n",
        "                actor_loss,\n",
        "                critic_loss,\n",
        "                importance_weighted_J_invt,\n",
        "                constrained_actor_loss)\n",
        "\n",
        "\n",
        "\n",
        "                # after gradient step but still in batch update lagrange\n",
        "                # return prob_ratio\n",
        "                # nu_grad = importance_weighted_J_invt.mean().detach()\n",
        "                # print(nu_grad)\n",
        "\n",
        "\n",
        "\n",
        "                # self.nu = T.clamp(self.nu - self.alpha_dual*nu_grad, min=0.0)\n",
        "                # print(self.nu.item())\n",
        "                # nu=self.nu.item()\n",
        "                # print(constrained_actor_loss)\n",
        "                # print(nu)\n",
        "\n",
        "\n",
        "\n",
        "        # Learn barrier function\n",
        "\n",
        "\n",
        "        # nu_grad = importance_weighted_J_invt.mean().detach()\n",
        "        # self.nu = T.clamp(self.nu - self.alpha_dual * nu_grad, min=0.0)\n",
        "        # barrier_loss = self.learn_barrier()\n",
        "        # Clear memories\n",
        "        # self.learn_barrier()\n",
        "        self.memory.clear_memory()\n",
        "        # self.barrier_memory = {\n",
        "        #     'initial_states': [],\n",
        "        #     'unsafe_states': [],\n",
        "        #     'states': [],\n",
        "        #     'next_states': []\n",
        "        # }\n",
        "\n",
        "        # return prob_ratio\n",
        "\n",
        "    def save_models(self):\n",
        "        print('...saving models...')\n",
        "        self.actor.save_checkpoint()\n",
        "        self.critic.save_checkpoint()\n",
        "\n",
        "    def load_models(self):\n",
        "        print('...loading models...')\n",
        "        self.actor.load_checkpoint()\n",
        "        self.critic.load_checkpoint()\n",
        "\n",
        "    def log_training_metrics(self, writer, step,\n",
        "                        actor_loss, critic_loss,\n",
        "                        importance_weighted_J_invt, constrained_actor_loss):\n",
        "      # \"\"\"Log training metrics to TensorBoard\"\"\"\n",
        "      # Loss components\n",
        "      writer.add_scalar('Losses/Actor_Loss', actor_loss.item(), step)\n",
        "      writer.add_scalar('Losses/Critic_Loss', critic_loss.item(), step)\n",
        "      # writer.add_scalar('Losses/Barrier_Loss', barrier_loss, step)\n",
        "      writer.add_scalar('Losses/Constrained_Actor_Loss', constrained_actor_loss.item(), step)\n",
        "\n",
        "      # Lagrangian metrics\n",
        "      writer.add_scalar('Lagrangian/Nu_Value', self.nu.item(), step)\n",
        "      writer.add_scalar('Lagrangian/Importance_Weighted_Constraint',\n",
        "                      importance_weighted_J_invt.item(), step)\n",
        "      # writer.add_scalar('Lagrangian/prob_ratio', prob_ratio.item() ,step)\n",
        "\n",
        "    def evaluate_barrier(self, state):\n",
        "        with T.no_grad():\n",
        "            state_tensor = T.tensor([state], dtype=T.float).to(self.barrier.device)\n",
        "            return self.barrier(state_tensor).item()"
      ],
      "metadata": {
        "id": "jDkwjw8mkiNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Redefine reward function\n",
        "class BoptestGymEnvCustomReward(BoptestGymEnv):\n",
        "    '''Define a custom reward for this building\n",
        "\n",
        "    '''\n",
        "    def get_reward(self):\n",
        "        '''Custom reward function. To expedite learning, we use a clipped reward\n",
        "        function that has a value of 1 when there is no increase in discomfort\n",
        "        and 0 otherwise. We use the BOPTEST `GET /kpis` API call to compute the\n",
        "        total cummulative discomfort from the beginning of the episode. Note\n",
        "        that this is the true value that BOPTEST uses when evaluating\n",
        "        controllers.\n",
        "\n",
        "        '''\n",
        "        # Compute BOPTEST core kpis\n",
        "        kpis = requests.get('{0}/kpi/{1}'.format(self.url, self.testid)).json()['payload']\n",
        "        # Calculate objective integrand function as the total discomfort\n",
        "        objective_integrand = 10*kpis['ener_tot']+ 10*kpis['cost_tot']\n",
        "        # Give reward if there is not immediate increment in discomfort\n",
        "        # if objective_integrand == self.objective_integrand:\n",
        "        #   reward=1\n",
        "        # else:\n",
        "        #   reward=0\n",
        "        # Record current objective integrand for next evaluatio\n",
        "        reward = -(objective_integrand - self.objective_integrand)\n",
        "        self.objective_integrand = objective_integrand\n",
        "        return reward"
      ],
      "metadata": {
        "id": "Bsu1dJltkiP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# env2.stop()\n",
        "import random\n",
        "\n",
        "# Seed for random starting times of episodes\n",
        "seed = 123456\n",
        "random.seed(seed)\n",
        "# Seed for random exploration and epsilon-greedy schedule\n",
        "np.random.seed(seed)\n",
        "\n",
        "# Winter period goes from December 21 (day 355) to March 20 (day 79)\n",
        "# Temperature setpoints\n",
        "lower_setp = 18 + 273.15\n",
        "upper_setp = 21 + 273.15\n",
        "# Instantiate environment\n",
        "env2 = BoptestGymEnvCustomReward(url                   = url,\n",
        "                                testcase              = 'bestest_hydronic_heat_pump',\n",
        "                                actions               = ['oveHeaPumY_u'],\n",
        "                                observations          = {'reaTZon_y':(lower_setp,upper_setp),'Occupancy[1]':(0,5)},\n",
        "                                predictive_period    = 0,\n",
        "                                random_start_time     = True,\n",
        "                                max_episode_length    = 2*24*3600,\n",
        "                                warmup_period         = 24*3600,\n",
        "                                step_period           = 3600,\n",
        "                                render_episodes       = True)\n",
        "env2 = NormalizedObservationWrapper(env2)\n",
        "env2 = DiscretizedActionWrapper(env2, n_bins_act=12)\n",
        "N = 24\n",
        "batch_size = 6\n",
        "n_epochs = 10\n",
        "alpha = 0.001\n",
        "agent = PPOAgent1_epch(n_actions=env2.action_space.n, batch_size=batch_size,\n",
        "                    alpha=alpha, n_epochs=n_epochs,\n",
        "                    input_dims=1)\n",
        "n_games = 10\n",
        "figure_file = 'plots/cartpole.png'\n",
        "num_episodes= 100\n",
        "barrier_only_episodes=20\n",
        "best_score = env2.reward_range[0]\n",
        "score_history = []\n",
        "\n",
        "learn_iters = 0\n",
        "avg_score = 0\n",
        "n_steps = 0"
      ],
      "metadata": {
        "id": "_0tHQY6gkiT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "writer = SummaryWriter()\n",
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "zN2LFAPlkiWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agent without pre-trained barrier certificate"
      ],
      "metadata": {
        "id": "zDCcyN1VlGSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import numpy as np\n",
        "\n",
        "# Create a SummaryWriter at the start of your training\n",
        "writer = SummaryWriter('runs/safety_ppo_experiment')\n",
        "\n",
        "# Initialize tracking variables\n",
        "n_steps = 0\n",
        "learn_iters = 0\n",
        "score_history = []\n",
        "best_score = float('-inf')\n",
        "\n",
        "for i in range(num_episodes):\n",
        "    observation, _ = env2.reset()\n",
        "    done = False\n",
        "    score = 0\n",
        "    episode_steps = 0\n",
        "\n",
        "    # For tracking episode-specific safety metrics\n",
        "    episode_unsafe_visits = 0\n",
        "    episode_safe_visits = 0\n",
        "\n",
        "    while not done:\n",
        "        action, prob, val = agent.choose_action(observation)\n",
        "        next_observation, reward, terminated, truncated, info = env2.step(action)\n",
        "        done = terminated or truncated\n",
        "        n_steps += 1\n",
        "        episode_steps += 1\n",
        "        score += reward\n",
        "\n",
        "        # Track safety violations within episode\n",
        "        if agent.is_unsafe(np.array(observation)):\n",
        "            episode_unsafe_visits += 1\n",
        "        if agent.is_safe(np.array(observation)):\n",
        "            episode_safe_visits += 1\n",
        "\n",
        "        agent.remember(observation, action, prob, val, reward, done, next_observation)\n",
        "\n",
        "        if n_steps % 48 == 0:\n",
        "            # Log before learning\n",
        "            writer.add_scalar('Barrier/States/Safe_Count',\n",
        "                            len(agent.barrier_memory['initial_states']), n_steps)\n",
        "            writer.add_scalar('Barrier/States/Unsafe_Count',\n",
        "                            len(agent.barrier_memory['unsafe_states']), n_steps)\n",
        "            writer.add_scalar('Barrier/States/Total_Transitions',\n",
        "                            len(agent.barrier_memory['next_states']), n_steps)\n",
        "\n",
        "            # Learning steps\n",
        "            agent.learn_barrier()\n",
        "            agent.learn()\n",
        "            learn_iters += 1\n",
        "\n",
        "        observation = next_observation\n",
        "\n",
        "    # Episode completed - log episode-level metrics\n",
        "    score_history.append(score)\n",
        "    avg_score = np.mean(score_history[-100:])\n",
        "\n",
        "    # Performance metrics\n",
        "    writer.add_scalar('Performance/Episode_Score', score, i)\n",
        "    writer.add_scalar('Performance/Average_Score', avg_score, i)\n",
        "    writer.add_scalar('Performance/Episode_Length', episode_steps, i)\n",
        "\n",
        "    # Safety metrics per episode\n",
        "    writer.add_scalar('Safety/Unsafe_Visits_Per_Episode', episode_unsafe_visits, i)\n",
        "    writer.add_scalar('Safety/Safe_Visits_Per_Episode', episode_safe_visits, i)\n",
        "    safety_ratio = episode_safe_visits / (episode_safe_visits + episode_unsafe_visits)\n",
        "    writer.add_scalar('Safety/Safety_Ratio', safety_ratio, i)\n",
        "\n",
        "    # Learning metrics\n",
        "    writer.add_scalar('Training/Learning_Steps', learn_iters, i)\n",
        "    writer.add_scalar('Training/Total_Steps', n_steps, i)\n",
        "\n",
        "    print(f'Episode {i+1}')\n",
        "    print(f'Score: {score:.2f}, Avg Score: {avg_score:.2f}')\n",
        "    print(f'Time Steps: {n_steps}, Learning Steps: {learn_iters}')\n",
        "    print(f'Safe states: {len(agent.barrier_memory[\"initial_states\"])}')\n",
        "    print(f'Unsafe states: {len(agent.barrier_memory[\"unsafe_states\"])}')\n",
        "    print(f'Total transitions: {len(agent.barrier_memory[\"next_states\"])}')\n",
        "\n",
        "    if avg_score > best_score:\n",
        "        best_score = avg_score\n",
        "\n",
        "print(f'Training completed. Final average score: {avg_score:.2f}')\n",
        "writer.close()\n",
        "env2.close()"
      ],
      "metadata": {
        "id": "wMvLzmskk8cz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(num_episodes):\n",
        "        observation, _ = env.reset()\n",
        "        done = False\n",
        "        score = 0\n",
        "        episode_steps = 0\n",
        "\n",
        "        # For tracking episode-specific safety metrics\n",
        "        episode_unsafe_visits = 0\n",
        "        episode_safe_visits = 0\n",
        "\n",
        "        while not done:\n",
        "            # Select action\n",
        "            action, prob, val = agent.choose_action(observation)\n",
        "\n",
        "            # Take step in environment\n",
        "            next_observation, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            # Update counters\n",
        "            n_steps += 1\n",
        "            episode_steps += 1\n",
        "            score += reward\n",
        "\n",
        "            # Track safety violations\n",
        "            if agent.is_unsafe(np.array(observation)):\n",
        "                episode_unsafe_visits += 1\n",
        "            if agent.is_safe(np.array(observation)):\n",
        "                episode_safe_visits += 1\n",
        "\n",
        "            # Store transition\n",
        "            agent.remember(observation, action, prob, val, reward, done, next_observation)\n",
        "\n",
        "            # Periodic learning\n",
        "            if n_steps % learning_freq == 0:\n",
        "                # Log current state\n",
        "                writer.add_scalar('Barrier/States/Safe_Count',\n",
        "                                len(agent.barrier_memory['initial_states']), n_steps)\n",
        "                writer.add_scalar('Barrier/States/Unsafe_Count',\n",
        "                                len(agent.barrier_memory['unsafe_states']), n_steps)\n",
        "                writer.add_scalar('Barrier/States/Total_Transitions',\n",
        "                                len(agent.barrier_memory['next_states']), n_steps)\n",
        "\n",
        "                # Learning - first 20 episodes only barrier learning, after that both\n",
        "                agent.learn_barrier()  # Always learn barrier function\n",
        "\n",
        "                if i >= barrier_only_episodes:  # After first 20 episodes, also learn policy\n",
        "                    agent.learn()\n",
        "\n",
        "                learn_iters += 1\n",
        "\n",
        "            observation = next_observation\n",
        "\n",
        "        # Episode complete - log episode-level metrics\n",
        "        score_history.append(score)\n",
        "        avg_score = np.mean(score_history[-100:]) if len(score_history) > 0 else score\n",
        "\n",
        "        # Performance metrics\n",
        "        writer.add_scalar('Performance/Episode_Score', score, i)\n",
        "        writer.add_scalar('Performance/Average_Score', avg_score, i)\n",
        "        writer.add_scalar('Performance/Episode_Length', episode_steps, i)\n",
        "\n",
        "        # Safety metrics\n",
        "        writer.add_scalar('Safety/Unsafe_Visits_Per_Episode', episode_unsafe_visits, i)\n",
        "        writer.add_scalar('Safety/Safe_Visits_Per_Episode', episode_safe_visits, i)\n",
        "        safety_ratio = episode_safe_visits / max(1, (episode_safe_visits + episode_unsafe_visits))\n",
        "        writer.add_scalar('Safety/Safety_Ratio', safety_ratio, i)\n",
        "\n",
        "        # Learning metrics\n",
        "        writer.add_scalar('Training/Learning_Steps', learn_iters, i)\n",
        "        writer.add_scalar('Training/Total_Steps', n_steps, i)\n",
        "\n",
        "        print(f'Episode {i+1}')\n",
        "        print(f'Score: {score:.2f}, Avg Score: {avg_score:.2f}')\n",
        "        print(f'Time Steps: {n_steps}, Learning Steps: {learn_iters}')\n",
        "        print(f'Safe states: {len(agent.barrier_memory[\"initial_states\"])}')\n",
        "        print(f'Unsafe states: {len(agent.barrier_memory[\"unsafe_states\"])}')\n",
        "        print(f'Total transitions: {len(agent.barrier_memory[\"next_states\"])}')\n",
        "\n",
        "        # Current learning phase\n",
        "        if i < barrier_only_episodes:\n",
        "            print(\"Current phase: BARRIER LEARNING ONLY\")\n",
        "        else:\n",
        "            print(\"Current phase: BARRIER AND POLICY LEARNING\")\n",
        "\n",
        "        # Save if best model\n",
        "        if avg_score > best_score:\n",
        "            best_score = avg_score\n",
        "            agent.save_models()\n",
        "\n",
        "    print(f'Training completed. Final average score: {avg_score:.2f}')\n",
        "    writer.close()\n",
        "    env.close()"
      ],
      "metadata": {
        "id": "zWLEHmH2kidi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}